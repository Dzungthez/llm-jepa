{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85aadedd",
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "import os\n",
        "\n",
        "# Set your HuggingFace token as an environment variable: export HF_TOKEN=\"your_token_here\"\n",
        "# Or use: huggingface-cli login\n",
        "hf_token = os.environ.get(\"HF_TOKEN\")\n",
        "if hf_token:\n",
        "    login(token=hf_token)\n",
        "else:\n",
        "    print(\"Warning: HF_TOKEN not found in environment variables. Using huggingface-cli login instead.\")\n",
        "    login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "71b2b040",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using vLLM for fast inference!\n"
          ]
        }
      ],
      "source": [
        "from vllm import LLM, SamplingParams\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# model_name = \"Qwen/Qwen2.5-Math-1.5B-Instruct\"\n",
        "# model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"\n",
        "\n",
        "print(\"Using vLLM for fast inference!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "8fd5ba7f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 01-02 09:03:38 [utils.py:253] non-default args: {'trust_remote_code': True, 'max_model_len': 1024, 'disable_log_stats': True, 'model': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-7B'}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 01-02 09:03:38 [model.py:514] Resolved architecture: Qwen2ForCausalLM\n",
            "INFO 01-02 09:03:38 [model.py:1661] Using max model len 1024\n",
            "INFO 01-02 09:03:45 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
            "\u001b[0;36m(EngineCore_DP0 pid=2114)\u001b[0;0m INFO 01-02 09:03:46 [core.py:93] Initializing a V1 LLM engine (v0.13.0) with config: model='deepseek-ai/DeepSeek-R1-Distill-Qwen-7B', speculative_config=None, tokenizer='deepseek-ai/DeepSeek-R1-Distill-Qwen-7B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=1024, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=deepseek-ai/DeepSeek-R1-Distill-Qwen-7B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=2114)\u001b[0;0m INFO 01-02 09:03:47 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.20.4.118:62972 backend=nccl\n",
            "\u001b[0;36m(EngineCore_DP0 pid=2114)\u001b[0;0m INFO 01-02 09:03:47 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "\u001b[0;36m(EngineCore_DP0 pid=2114)\u001b[0;0m INFO 01-02 09:03:47 [gpu_model_runner.py:3562] Starting to load model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B...\n",
            "\u001b[0;36m(EngineCore_DP0 pid=2114)\u001b[0;0m INFO 01-02 09:03:48 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5d5221fb8d9e4f0a80fe9030e5b8dd5e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[0;36m(EngineCore_DP0 pid=2114)\u001b[0;0m INFO 01-02 09:03:53 [default_loader.py:308] Loading weights took 4.55 seconds\n",
            "\u001b[0;36m(EngineCore_DP0 pid=2114)\u001b[0;0m INFO 01-02 09:03:53 [gpu_model_runner.py:3659] Model loading took 14.2717 GiB memory and 5.136926 seconds\n",
            "\u001b[0;36m(EngineCore_DP0 pid=2114)\u001b[0;0m INFO 01-02 09:04:01 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/7f016add07/rank_0_0/backbone for vLLM's torch.compile\n",
            "\u001b[0;36m(EngineCore_DP0 pid=2114)\u001b[0;0m INFO 01-02 09:04:01 [backends.py:703] Dynamo bytecode transform time: 7.38 s\n",
            "\u001b[0;36m(EngineCore_DP0 pid=2114)\u001b[0;0m INFO 01-02 09:04:07 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 8192) from the cache, took 3.658 s\n",
            "\u001b[0;36m(EngineCore_DP0 pid=2114)\u001b[0;0m INFO 01-02 09:04:07 [monitor.py:34] torch.compile takes 11.04 s in total\n",
            "\u001b[0;36m(EngineCore_DP0 pid=2114)\u001b[0;0m INFO 01-02 09:04:08 [gpu_worker.py:375] Available KV cache memory: 55.57 GiB\n",
            "\u001b[0;36m(EngineCore_DP0 pid=2114)\u001b[0;0m INFO 01-02 09:04:08 [kv_cache_utils.py:1291] GPU KV cache size: 1,040,544 tokens\n",
            "\u001b[0;36m(EngineCore_DP0 pid=2114)\u001b[0;0m INFO 01-02 09:04:08 [kv_cache_utils.py:1296] Maximum concurrency for 1,024 tokens per request: 1016.16x\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51/51 [00:01<00:00, 26.30it/s]\n",
            "Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:01<00:00, 32.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[0;36m(EngineCore_DP0 pid=2114)\u001b[0;0m INFO 01-02 09:04:12 [gpu_model_runner.py:4587] Graph capturing finished in 4 secs, took 0.54 GiB\n",
            "\u001b[0;36m(EngineCore_DP0 pid=2114)\u001b[0;0m INFO 01-02 09:04:12 [core.py:259] init engine (profile, create kv cache, warmup model) took 18.54 seconds\n",
            "INFO 01-02 09:04:13 [llm.py:360] Supported tasks: ['generate']\n",
            "vLLM model loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "# Load model using vLLM (much faster than transformers)\n",
        "llm = LLM(\n",
        "    model=model_name,\n",
        "    trust_remote_code=True,\n",
        "    max_model_len=1024,  # Adjust based on your GPU memory\n",
        "    gpu_memory_utilization=0.9,  # Use 90% of GPU memory\n",
        ")\n",
        "\n",
        "# Load tokenizer for formatting\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "\n",
        "# Set pad token if not present\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(f\"vLLM model loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "0c01e070",
      "metadata": {},
      "outputs": [],
      "source": [
        "sampling_params = SamplingParams(\n",
        "    temperature=0.0,  # Greedy decoding\n",
        "    max_tokens=1024,\n",
        "    top_p=1.0,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "02619716",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 1319 examples from GSM8K test set\n"
          ]
        }
      ],
      "source": [
        "# Load GSM8K test dataset\n",
        "from datasets import load_dataset\n",
        "import json\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load GSM8K test set\n",
        "dataset = load_dataset(\"openai/gsm8k\", \"main\", split=\"test\")\n",
        "print(f\"Loaded {len(dataset)} examples from GSM8K test set\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "a90d1446",
      "metadata": {},
      "outputs": [],
      "source": [
        "trainset = load_dataset(\"openai/gsm8k\", \"main\", split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "d9c369d9",
      "metadata": {},
      "outputs": [],
      "source": [
        "math_dataset = load_dataset(\"qwedsacf/competition_math\", \"default\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "c3f4edcd",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation functions defined\n"
          ]
        }
      ],
      "source": [
        "# GSM8K evaluation patterns\n",
        "gsm8k_pattern = re.compile(r\"\\n#### (.+)$\")  # Original GSM8K format\n",
        "boxed_pattern = re.compile(r\"\\\\boxed\\{([^}]+)\\}\")  # Qwen model output format\n",
        "hash_pattern = re.compile(r\"####\\s*(.+?)(?:\\n|$)\")  # #### format anywhere in text\n",
        "\n",
        "def extract_number(text):\n",
        "    \"\"\"Extract numeric value from text, ignoring units, currency symbols, etc.\n",
        "    \n",
        "    Examples:\n",
        "        '$65,000' -> '65000'\n",
        "        '\\$18' -> '18'\n",
        "        '18 dollars' -> '18'\n",
        "        '-5.5%' -> '-5.5'\n",
        "        '1/2' -> '0.5'\n",
        "    \"\"\"\n",
        "    if text is None:\n",
        "        return None\n",
        "    \n",
        "    text = str(text).strip()\n",
        "    \n",
        "    # Handle fractions (e.g., \"1/2\" -> \"0.5\")\n",
        "    fraction_match = re.match(r'^(-?\\d+)/(\\d+)$', text.replace(' ', ''))\n",
        "    if fraction_match:\n",
        "        numerator = float(fraction_match.group(1))\n",
        "        denominator = float(fraction_match.group(2))\n",
        "        if denominator != 0:\n",
        "            return str(numerator / denominator)\n",
        "    \n",
        "    # Remove currency symbols ($, â‚¬, Â£, Â¥, etc.)\n",
        "    text = re.sub(r'[\\$â‚¬Â£Â¥â‚¹]', '', text)\n",
        "    \n",
        "    # Remove common units (words)\n",
        "    text = re.sub(r'\\b(dollars?|cents?|years?|months?|days?|hours?|minutes?|seconds?|meters?|km|cm|kg|g|lbs?|oz|percent|%)\\b', '', text, flags=re.IGNORECASE)\n",
        "    \n",
        "    # Remove commas from numbers\n",
        "    text = text.replace(',', '')\n",
        "    \n",
        "    # Extract the number (handles negative, decimals)\n",
        "    # This pattern matches: optional minus, digits, optional decimal point and more digits\n",
        "    number_match = re.search(r'-?\\d+(?:\\.\\d+)?', text)\n",
        "    \n",
        "    if number_match:\n",
        "        return number_match.group(0)\n",
        "    \n",
        "    return None\n",
        "\n",
        "def extract_gsm8k_answer(text):\n",
        "    \"\"\"Extract answer from GSM8K format (#### answer at end)\"\"\"\n",
        "    match = re.search(gsm8k_pattern, text)\n",
        "    if not match:\n",
        "        return None\n",
        "    answer_text = match.group(1).strip()\n",
        "    return extract_number(answer_text)\n",
        "\n",
        "def extract_boxed_answer(text):\n",
        "    \"\"\"Extract answer from boxed format (\\\\boxed{answer})\"\"\"\n",
        "    matches = boxed_pattern.findall(text)\n",
        "    if not matches:\n",
        "        return None\n",
        "    answer_text = matches[-1].strip()\n",
        "    return extract_number(answer_text)\n",
        "\n",
        "def extract_hash_answer(text):\n",
        "    \"\"\"Extract answer from #### format (anywhere in text)\"\"\"\n",
        "    matches = hash_pattern.findall(text)\n",
        "    if not matches:\n",
        "        return None\n",
        "    answer_text = matches[-1].strip()\n",
        "    return extract_number(answer_text)\n",
        "\n",
        "def normalize_answer(answer):\n",
        "    \"\"\"Normalize numeric answer for comparison\"\"\"\n",
        "    if answer is None:\n",
        "        return None\n",
        "    \n",
        "    answer = str(answer).strip()\n",
        "    \n",
        "    # Try to convert to float and back to handle different representations\n",
        "    try:\n",
        "        # Convert to float to handle cases like \"5.0\" vs \"5\"\n",
        "        num = float(answer)\n",
        "        # If it's a whole number, convert to int to avoid \"5.0\" vs \"5\" mismatches\n",
        "        if num.is_integer():\n",
        "            return str(int(num))\n",
        "        else:\n",
        "            # Round to reasonable precision to avoid floating point issues\n",
        "            return f\"{num:.10g}\"  # 'g' format removes trailing zeros\n",
        "    except (ValueError, TypeError):\n",
        "        # If conversion fails, just return cleaned string\n",
        "        return answer.replace(',', '').strip()\n",
        "\n",
        "def eval_gsm8k(generated, ground_truth, extraction_func=None):\n",
        "    \"\"\"Evaluate GSM8K answer - handles multiple formats\n",
        "    \n",
        "    Args:\n",
        "        generated: The generated response text\n",
        "        ground_truth: The ground truth answer in GSM8K format\n",
        "        extraction_func: Function to extract answer from generated text\n",
        "                        If None, tries boxed, then hash, then gsm8k formats\n",
        "    \"\"\"\n",
        "    # Extract ground truth (GSM8K format with ####)\n",
        "    gt_answer = extract_gsm8k_answer(ground_truth)\n",
        "    \n",
        "    # Extract generated answer - try multiple formats\n",
        "    if extraction_func is not None:\n",
        "        gen_answer = extraction_func(generated)\n",
        "    else:\n",
        "        # Try boxed format first (for Qwen)\n",
        "        gen_answer = extract_boxed_answer(generated)\n",
        "        # If not found, try #### format\n",
        "        if gen_answer is None:\n",
        "            gen_answer = extract_hash_answer(generated)\n",
        "        # Last resort: try end-of-text GSM8K format\n",
        "        if gen_answer is None:\n",
        "            gen_answer = extract_gsm8k_answer(generated)\n",
        "    \n",
        "    # Normalize both answers\n",
        "    gt_answer_norm = normalize_answer(gt_answer)\n",
        "    gen_answer_norm = normalize_answer(gen_answer)\n",
        "    \n",
        "    # Compare normalized answers\n",
        "    is_correct = (gt_answer_norm == gen_answer_norm) if (gt_answer_norm and gen_answer_norm) else False\n",
        "    \n",
        "    return is_correct, gt_answer, gen_answer\n",
        "\n",
        "print(\"Evaluation functions defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "df47f8de",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "TESTING SYSTEM PROMPT\n",
            "================================================================================\n",
            "\n",
            "System Prompt:\n",
            "Please solve the problem step by step (separate steps with double newlines), but keep it short and put your final answer (do not include any other text or units) within \\boxed{}.\n",
            "\n",
            "Question:\n",
            "James decides to run 3 sprints 3 times a week.  He runs 60 meters each sprint.  How many total meters does he run a week?\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8473f25140a548bd980402e2c6255659",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f4b4cc33032e46f49ab2154aa0464818",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "GENERATED RESPONSE:\n",
            "First, determine how many sprints James runs each week. He runs 3 sprints 3 times a week, so that's 3 multiplied by 3, which equals 9 sprints per week.\n",
            "\n",
            "Next, calculate the total distance he covers in one week by multiplying the number of sprints by the distance of each sprint. Each sprint is 60 meters, so 9 sprints multiplied by 60 meters per sprint equals 540 meters.\n",
            "\n",
            "Therefore, James runs a total of 540 meters each week.\n",
            "</think>\n",
            "\n",
            "**Solution:**\n",
            "\n",
            "1. **Determine the total number of sprints James runs in a week:**\n",
            "   \n",
            "   James runs **3 sprints** **3 times** a week.\n",
            "   \n",
            "   \\[\n",
            "   \\text{Total sprints per week} = 3 \\times 3 = 9 \\text{ sprints}\n",
            "   \\]\n",
            "\n",
            "2. **Calculate the total distance James runs in a week:**\n",
            "   \n",
            "   Each sprint is **60 meters**.\n",
            "   \n",
            "   \\[\n",
            "   \\text{Total distance} = 9 \\text{ sprints} \\times 60 \\text{ meters/sprint} = 540 \\text{ meters}\n",
            "   \\]\n",
            "\n",
            "**Final Answer:**\n",
            "\n",
            "\\[\n",
            "\\boxed{540}\n",
            "\\]\n",
            "\n",
            "================================================================================\n",
            "Ground Truth Answer: 540\n",
            "Generated Answer: 540\n",
            "Correct: âœ“\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "test_system_prompt = \"\"\"Please solve the problem step by step (separate steps with double newlines), but keep it short and put your final answer (do not include any other text or units) within \\\\boxed{}.\"\"\"\n",
        "\n",
        "# test_system_prompt = \"Please solve step by step and put your final numerical answer (do not include any other text or units) after #### \"\n",
        "# test_system_prompt = \"Solve this problem step by step. Put your final answer in the format: #### [answer]\"\n",
        "\n",
        "# Choose a question to test (or use a custom one):\n",
        "test_question = dataset[3]['question']  # First question from GSM8K\n",
        "test_ground_truth = dataset[3]['answer']\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"TESTING SYSTEM PROMPT\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nSystem Prompt:\\n{test_system_prompt}\\n\")\n",
        "print(f\"Question:\\n{test_question}\\n\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "# Create messages\n",
        "test_messages = [\n",
        "    {\"role\": \"system\", \"content\": test_system_prompt},\n",
        "    {\"role\": \"user\", \"content\": test_question}\n",
        "]\n",
        "\n",
        "# Format and generate\n",
        "test_prompt = tokenizer.apply_chat_template(test_messages, tokenize=False, add_generation_prompt=True)\n",
        "test_outputs = llm.generate([test_prompt], sampling_params)\n",
        "test_response = test_outputs[0].outputs[0].text.strip()\n",
        "\n",
        "# Evaluate\n",
        "is_correct, gt_ans, gen_ans = eval_gsm8k(test_response, test_ground_truth)\n",
        "\n",
        "print(f\"\\nGENERATED RESPONSE:\")\n",
        "print(test_response)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(f\"Ground Truth Answer: {gt_ans}\")\n",
        "print(f\"Generated Answer: {gen_ans}\")\n",
        "print(f\"Correct: {'âœ“' if is_correct else 'âœ—'}\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "98033444",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preparing prompts for batch inference...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Preparing data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1319/1319 [00:00<00:00, 12133.99it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Prepared 1319 prompts\n",
            "Starting batch inference with vLLM (this will be MUCH faster!)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Prepare all prompts first (for batch inference with vLLM)\n",
        "output_file = \"gsm8k_generations.jsonl\"\n",
        "\n",
        "# System prompt for math reasoning\n",
        "# system_prompt = \"Please reason step by step, and put your final answer within \\\\boxed{}.\"\n",
        "system_prompt = test_system_prompt\n",
        "\n",
        "print(\"Preparing prompts for batch inference...\")\n",
        "prompts = []\n",
        "questions = []\n",
        "ground_truths = []\n",
        "\n",
        "for example in tqdm(dataset, desc=\"Preparing data\"):\n",
        "    question = example['question']\n",
        "    ground_truth_answer = example['answer']\n",
        "    \n",
        "    # Create messages in chat format\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": question}\n",
        "    ]\n",
        "    \n",
        "    # Format conversation using tokenizer's chat template\n",
        "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    \n",
        "    prompts.append(prompt)\n",
        "    questions.append(question)\n",
        "    ground_truths.append(ground_truth_answer)\n",
        "\n",
        "print(f\"\\nPrepared {len(prompts)} prompts\")\n",
        "print(f\"Starting batch inference with vLLM (this will be MUCH faster!)...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "ee42d329",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating responses with vLLM...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a09b74299eff4ae7874f88e181d61a33",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Adding requests:   0%|          | 0/1319 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "951e97861ea84d22a7003559b6f7cb1f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processed prompts:   0%|          | 0/1319 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/sâ€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated 1319 responses!\n",
            "Processing results...\n"
          ]
        }
      ],
      "source": [
        "# Generate responses using vLLM (batch inference - FAST!)\n",
        "print(\"Generating responses with vLLM...\")\n",
        "outputs = llm.generate(prompts, sampling_params)\n",
        "\n",
        "print(f\"Generated {len(outputs)} responses!\")\n",
        "print(\"Processing results...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "748662ed",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1319/1319 [00:00<00:00, 40734.59it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "Evaluation Complete!\n",
            "==================================================\n",
            "Total examples: 1319\n",
            "Correct: 1158\n",
            "Accuracy: 0.8779 (87.79%)\n",
            "Results saved to: gsm8k_generations.jsonl\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Process outputs and evaluate\n",
        "results = []\n",
        "correct_count = 0\n",
        "total_count = 0\n",
        "\n",
        "with open(output_file, 'w') as f:\n",
        "    for idx, output in enumerate(tqdm(outputs, desc=\"Evaluating\")):\n",
        "        try:\n",
        "            # Extract generated text\n",
        "            generated_response = output.outputs[0].text.strip()\n",
        "            \n",
        "            # Get corresponding question and ground truth\n",
        "            question = questions[idx]\n",
        "            ground_truth_answer = ground_truths[idx]\n",
        "            \n",
        "            # Evaluate\n",
        "            is_correct, gt_answer, gen_answer = eval_gsm8k(generated_response, ground_truth_answer)\n",
        "            \n",
        "            if is_correct:\n",
        "                correct_count += 1\n",
        "            total_count += 1\n",
        "            \n",
        "            # Save result in the same format as evaluate.py\n",
        "            result = {\n",
        "                \"question\": question,\n",
        "                \"ground_truth\": ground_truth_answer,\n",
        "                \"generated_response\": generated_response,\n",
        "                \"gt_answer\": gt_answer,\n",
        "                \"gen_answer\": gen_answer,\n",
        "                \"correct\": is_correct,\n",
        "                \"index\": idx\n",
        "            }\n",
        "            results.append(result)\n",
        "            \n",
        "            # Write to JSONL file\n",
        "            f.write(json.dumps(result) + '\\n')\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error at index {idx}: {e}\")\n",
        "            result = {\n",
        "                \"question\": questions[idx],\n",
        "                \"ground_truth\": ground_truths[idx],\n",
        "                \"generated_response\": \"\",\n",
        "                \"gt_answer\": None,\n",
        "                \"gen_answer\": None,\n",
        "                \"correct\": False,\n",
        "                \"index\": idx,\n",
        "                \"error\": str(e)\n",
        "            }\n",
        "            results.append(result)\n",
        "            f.write(json.dumps(result) + '\\n')\n",
        "            total_count += 1\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = correct_count / total_count if total_count > 0 else 0\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(f\"Evaluation Complete!\")\n",
        "print(f\"{'='*50}\")\n",
        "print(f\"Total examples: {total_count}\")\n",
        "print(f\"Correct: {correct_count}\")\n",
        "print(f\"Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "print(f\"Results saved to: {output_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "310a4d56",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "Example Results (first 3):\n",
            "================================================================================\n",
            "\n",
            "--- Example 1 ---\n",
            "Question: Janetâ€™s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for ...\n",
            "Ground Truth Answer: 18\n",
            "Generated Answer: 18\n",
            "Correct: True\n",
            "Full Generation: First, I need to determine how many duck eggs Janet has each day. She lays 16 eggs per day.\n",
            "\n",
            "Next, I'll account for the eggs she uses. She eats 3 eggs for breakfast and uses 4 eggs to bake muffins, to...\n",
            "\n",
            "\n",
            "--- Example 2 ---\n",
            "Question: A robe takes 2 bolts of blue fiber and half that much white fiber.  How many bolts in total does it ...\n",
            "Ground Truth Answer: 3\n",
            "Generated Answer: 3\n",
            "Correct: True\n",
            "Full Generation: First, identify the amount of blue fiber needed for the robe, which is 2 bolts.\n",
            "\n",
            "Next, determine the amount of white fiber required, which is half of the blue fiber. Half of 2 bolts is 1 bolt.\n",
            "\n",
            "Finall...\n",
            "\n",
            "\n",
            "--- Example 3 ---\n",
            "Question: Josh decides to try flipping a house.  He buys a house for $80,000 and then puts in $50,000 in repai...\n",
            "Ground Truth Answer: 70000\n",
            "Generated Answer: 65000\n",
            "Correct: False\n",
            "Full Generation: First, I need to determine the total cost Josh incurred in purchasing and repairing the house.\n",
            "\n",
            "He bought the house for $80,000 and spent an additional $50,000 on repairs. Adding these amounts togethe...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Display some example results\n",
        "print(\"=\"*80)\n",
        "print(\"Example Results (first 3):\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for i, result in enumerate(results[:3]):\n",
        "    print(f\"\\n--- Example {i+1} ---\")\n",
        "    print(f\"Question: {result['question'][:100]}...\")\n",
        "    print(f\"Ground Truth Answer: {result['gt_answer']}\")\n",
        "    print(f\"Generated Answer: {result['gen_answer']}\")\n",
        "    print(f\"Correct: {result['correct']}\")\n",
        "    print(f\"Full Generation: {result['generated_response'][:200]}...\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "678543eb",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "SYNTHETIC DATA GENERATION CONFIGURATION\n",
            "================================================================================\n",
            "Max retries per question: 5\n",
            "System prompt: Please solve the problem step by step (separate steps with double newlines), but keep it short and put your final answer (do not include any other text or units) within \\boxed{}.\n",
            "Output file: gsm8k_synthetic_data.jsonl\n",
            "Max examples: All\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# ========================================================================\n",
        "# SYNTHETIC DATA GENERATION WITH RETRY MECHANISM\n",
        "# ========================================================================\n",
        "\n",
        "# Configuration\n",
        "MAX_RETRIES = 5  # Maximum number of generation attempts per question\n",
        "SYNTHETIC_OUTPUT_FILE = \"gsm8k_synthetic_data.jsonl\"\n",
        "# synthetic_system_prompt = r\"Please reason step by step, and put your final answer within \\boxed{}.\"\n",
        "synthetic_system_prompt = test_system_prompt\n",
        "\n",
        "# You can also limit the number of examples to process (None = all)\n",
        "MAX_EXAMPLES_FOR_SYNTHETIC = None  # Set to e.g. 100 for testing, None for full dataset\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"SYNTHETIC DATA GENERATION CONFIGURATION\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Max retries per question: {MAX_RETRIES}\")\n",
        "print(f\"System prompt: {synthetic_system_prompt}\")\n",
        "print(f\"Output file: {SYNTHETIC_OUTPUT_FILE}\")\n",
        "print(f\"Max examples: {MAX_EXAMPLES_FOR_SYNTHETIC or 'All'}\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f7cf47b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "SYNTHETIC DATA GENERATION CONFIGURATION\n",
            "================================================================================\n",
            "Max retries per question: 5\n",
            "System prompt: Please solve the problem step by step (separate steps with double newlines), but keep it short and put your final answer (do not include any other text or units) within \\boxed{}.\n",
            "Output file: gsm8k_synthetic_data.jsonl\n",
            "Max examples: All\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Generate synthetic data with BATCH retry mechanism (FAST!)\n",
        "import time\n",
        "\n",
        "# Prepare dataset\n",
        "synthetic_dataset = trainset if MAX_EXAMPLES_FOR_SYNTHETIC is None else dataset.select(range(min(MAX_EXAMPLES_FOR_SYNTHETIC, len(dataset))))\n",
        "\n",
        "print(f\"\\nStarting BATCH synthetic data generation for {len(synthetic_dataset)} examples...\")\n",
        "print(f\"Using batch inference with retry (max {MAX_RETRIES} attempts)\\n\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Prepare all data\n",
        "all_questions = []\n",
        "all_ground_truths = []\n",
        "all_prompts = []\n",
        "\n",
        "for example in synthetic_dataset:\n",
        "    question = example['question']\n",
        "    ground_truth = example['answer']\n",
        "    \n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": synthetic_system_prompt},\n",
        "        {\"role\": \"user\", \"content\": question}\n",
        "    ]\n",
        "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    \n",
        "    all_questions.append(question)\n",
        "    all_ground_truths.append(ground_truth)\n",
        "    all_prompts.append(prompt)\n",
        "\n",
        "# Track results\n",
        "results = [None] * len(synthetic_dataset)  # Store successful responses\n",
        "pending_indices = list(range(len(synthetic_dataset)))  # Indices still needing correct answer\n",
        "total_attempts = 0\n",
        "attempt_count = 0\n",
        "\n",
        "# Batch generation with retry\n",
        "while pending_indices and attempt_count < MAX_RETRIES:\n",
        "    attempt_count += 1\n",
        "    print(f\"\\nAttempt {attempt_count}/{MAX_RETRIES}: Generating for {len(pending_indices)} questions...\")\n",
        "    \n",
        "    # Get prompts for pending questions\n",
        "    pending_prompts = [all_prompts[i] for i in pending_indices]\n",
        "    \n",
        "    # Batch generate\n",
        "    outputs = llm.generate(pending_prompts, sampling_params)\n",
        "    total_attempts += len(pending_prompts)\n",
        "    \n",
        "    # Evaluate results\n",
        "    new_pending = []\n",
        "    for i, output_idx in enumerate(pending_indices):\n",
        "        response = outputs[i].outputs[0].text.strip()\n",
        "        \n",
        "        # Check if correct\n",
        "        is_correct, gt_ans, gen_ans = eval_gsm8k(response, all_ground_truths[output_idx])\n",
        "        \n",
        "        if is_correct:\n",
        "            # Store successful result\n",
        "            results[output_idx] = response\n",
        "        else:\n",
        "            # Still needs correct answer\n",
        "            new_pending.append(output_idx)\n",
        "    \n",
        "    pending_indices = new_pending\n",
        "    success_count = sum(1 for r in results if r is not None)\n",
        "    print(f\"  -> {success_count}/{len(synthetic_dataset)} correct ({success_count/len(synthetic_dataset)*100:.2f}%)\")\n",
        "\n",
        "# Create final dataset (only successful examples)\n",
        "synthetic_data = []\n",
        "success_count = 0\n",
        "failed_count = 0\n",
        "\n",
        "for idx in range(len(synthetic_dataset)):\n",
        "    if results[idx] is not None:\n",
        "        # Extract ground truth numeric answer for easy comparison\n",
        "        ground_truth_answer = extract_gsm8k_answer(all_ground_truths[idx])\n",
        "        \n",
        "        synthetic_entry = {\n",
        "            \"question\": all_questions[idx],\n",
        "            \"answer\": all_ground_truths[idx],  # Full GSM8K answer with #### format\n",
        "            \"ground_truth\": ground_truth_answer,  # Extracted numeric answer\n",
        "            \"deepseek_response\": results[idx]\n",
        "        }\n",
        "        synthetic_data.append(synthetic_entry)\n",
        "        success_count += 1\n",
        "    else:\n",
        "        failed_count += 1\n",
        "\n",
        "elapsed_time = time.time() - start_time\n",
        "\n",
        "# Print statistics\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"SYNTHETIC DATA GENERATION COMPLETE\")\n",
        "print(f\"{'='*80}\")\n",
        "print(f\"Total examples processed: {len(synthetic_dataset)}\")\n",
        "print(f\"Successful (found correct answer): {success_count} ({success_count/len(synthetic_dataset)*100:.2f}%)\")\n",
        "print(f\"Failed (no correct answer in {MAX_RETRIES} attempts): {failed_count} ({failed_count/len(synthetic_dataset)*100:.2f}%)\")\n",
        "print(f\"Total generation attempts: {total_attempts}\")\n",
        "print(f\"Average attempts per question: {total_attempts/len(synthetic_dataset):.2f}\")\n",
        "print(f\"Time elapsed: {elapsed_time:.2f}s ({elapsed_time/60:.2f} minutes)\")\n",
        "print(f\"Speed: {len(synthetic_dataset)/elapsed_time:.2f} examples/second\")\n",
        "print(f\"{'='*80}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "875b7ecb",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Saving synthetic data to gsm8k_synthetic_data.jsonl...\n",
            "âœ“ Saved 6862 successful entries to gsm8k_synthetic_data.jsonl\n",
            "\n",
            "File format (same as GSM8K + additional fields):\n",
            "  - question: The math problem\n",
            "  - answer: Full ground truth with #### format\n",
            "  - ground_truth: Extracted numeric answer (for easy comparison)\n",
            "  - deepseek_response: Model's correct response\n"
          ]
        }
      ],
      "source": [
        "# Save synthetic data to JSONL file\n",
        "print(f\"\\nSaving synthetic data to {SYNTHETIC_OUTPUT_FILE}...\")\n",
        "\n",
        "with open(SYNTHETIC_OUTPUT_FILE, 'w') as f:\n",
        "    for entry in synthetic_data:\n",
        "        f.write(json.dumps(entry) + '\\n')\n",
        "\n",
        "print(f\"âœ“ Saved {len(synthetic_data)} successful entries to {SYNTHETIC_OUTPUT_FILE}\")\n",
        "print(f\"\\nFile format (same as GSM8K + additional fields):\")\n",
        "print(f\"  - question: The math problem\")\n",
        "print(f\"  - answer: Full ground truth with #### format\")\n",
        "print(f\"  - ground_truth: Extracted numeric answer (for easy comparison)\")\n",
        "print(f\"  - deepseek_response: Model's correct response\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "19563e1a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Saving synthetic data to gsm8k_synthetic_data.jsonl...\n",
            "âœ“ Saved 6862 successful entries to gsm8k_synthetic_data.jsonl\n",
            "\n",
            "File format (same as GSM8K + deepseek_response):\n",
            "  - question: The math problem\n",
            "  - answer: Ground truth with #### format\n",
            "  - deepseek_response: Model's correct response\n"
          ]
        }
      ],
      "source": [
        "# Display examples of synthetic data\n",
        "print(\"=\"*80)\n",
        "print(\"SYNTHETIC DATA EXAMPLES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Show first few successful examples\n",
        "if len(synthetic_data) > 0:\n",
        "    print(f\"\\nðŸ“Š Example 1:\")\n",
        "    print(\"-\"*80)\n",
        "    example = synthetic_data[0]\n",
        "    print(f\"Question: {example['question'][:200]}...\")\n",
        "    print(f\"\\nGround Truth (extracted): {example['ground_truth']}\")\n",
        "    print(f\"\\nFull Answer: {example['answer'][:100]}...\")\n",
        "    print(f\"\\nDeepseek Response: {example['deepseek_response'][:400]}...\")\n",
        "    \n",
        "    if len(synthetic_data) > 1:\n",
        "        print(f\"\\n\\nðŸ“Š Example 2:\")\n",
        "        print(\"-\"*80)\n",
        "        example = synthetic_data[1]\n",
        "        print(f\"Question: {example['question'][:200]}...\")\n",
        "        print(f\"\\nGround Truth (extracted): {example['ground_truth']}\")\n",
        "        print(f\"\\nFull Answer: {example['answer'][:100]}...\")\n",
        "        print(f\"\\nDeepseek Response: {example['deepseek_response'][:400]}...\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "f3b650f3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "SYNTHETIC DATA EXAMPLES\n",
            "================================================================================\n",
            "\n",
            "ðŸ“Š Example 1:\n",
            "--------------------------------------------------------------------------------\n",
            "Question: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\n",
            "\n",
            "Ground Truth Answer: Natalia sold 48/2 = <<48/2=24>>24 clips in May.\n",
            "Natalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\n",
            "#### 72\n",
            "\n",
            "Deepseek Response: First, I note that Natalia sold 48 clips in April.\n",
            "\n",
            "In May, she sold half as many clips as she did in April, which is 24 clips.\n",
            "\n",
            "To find the total number of clips sold over both months, I add the clips sold in April and May: 48 + 24 = 72.\n",
            "\n",
            "Therefore, Natalia sold a total of 72 clips in April and May.\n",
            "</think>\n",
            "\n",
            "**Step 1:** Determine the number of clips Natalia sold in April.\n",
            "\\[\n",
            "\\text{Clips sold in April} = 48\n",
            "\\]\n",
            "\n",
            "**Step 2:** Calculate the number of clips sold in May, which is half of April's sale...\n",
            "\n",
            "\n",
            "ðŸ“Š Example 2:\n",
            "--------------------------------------------------------------------------------\n",
            "Question: Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?\n",
            "\n",
            "Ground Truth Answer: Weng earns 12/60 = $<<12/60=0.2>>0.2 per minute.\n",
            "Working 50 minutes, she earned 0.2 x 50 = $<<0.2*50=10>>10.\n",
            "#### 10\n",
            "\n",
            "Deepseek Response: First, I need to determine how much Weng earned for babysitting. She earns $12 per hour, and she worked for 50 minutes.\n",
            "\n",
            "Since her pay rate is given in hours, I should convert the 50 minutes into hours. There are 60 minutes in an hour, so 50 minutes is 50/60 hours, which simplifies to 5/6 of an hour.\n",
            "\n",
            "Next, I'll calculate her earnings by multiplying her hourly rate by the number of hours she worked. So, $12 multiplied by 5/6 equals $10.\n",
            "\n",
            "Therefore, Weng earned $10 for babysitting yesterday.\n",
            "</th...\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# How to load and use the synthetic data\n",
        "print(\"=\"*80)\n",
        "print(\"HOW TO USE THE SYNTHETIC DATA\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\"\"\n",
        "The synthetic data has been saved to: {SYNTHETIC_OUTPUT_FILE}\n",
        "\n",
        "Format (same as GSM8K dataset + additional fields):\n",
        "{{\n",
        "    \"question\": \"Math problem text\",\n",
        "    \"answer\": \"Solution with #### final_answer\",\n",
        "    \"ground_truth\": \"18\",  // Extracted numeric answer\n",
        "    \"deepseek_response\": \"Model's correct reasoning with \\\\boxed{{answer}}\"\n",
        "}}\n",
        "\n",
        "Only successful examples (where model got correct answer) are included.\n",
        "\n",
        "Example usage:\n",
        "\"\"\")\n",
        "\n",
        "print(\"# Load the synthetic data:\")\n",
        "print(\"from datasets import load_dataset\")\n",
        "print(f\"synthetic_ds = load_dataset('json', data_files='{SYNTHETIC_OUTPUT_FILE}')['train']\")\n",
        "print()\n",
        "print(\"# Access fields:\")\n",
        "print(\"for example in synthetic_ds:\")\n",
        "print(\"    question = example['question']\")\n",
        "print(\"    full_answer = example['answer']  # Full GSM8K format\")\n",
        "print(\"    ground_truth = example['ground_truth']  # Just the number\")\n",
        "print(\"    model_response = example['deepseek_response']\")\n",
        "print()\n",
        "print(\"# The ground_truth field makes comparison easier:\")\n",
        "print(\"# - No need to parse #### format\")\n",
        "print(\"# - Direct numeric comparison\")\n",
        "print(\"# - Useful for analysis and evaluation\")\n",
        "print()\n",
        "print(\"# Use cases:\")\n",
        "print(\"# 1. Fine-tuning - Use as training data\")\n",
        "print(\"# 2. Data augmentation - Add to existing datasets\")\n",
        "print(\"# 3. Distillation - Train smaller models\")\n",
        "print(\"# 4. Analysis - Study model reasoning patterns\")\n",
        "print(\"# 5. Evaluation - Easy comparison with ground_truth field\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
