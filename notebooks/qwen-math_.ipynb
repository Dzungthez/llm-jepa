{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85aadedd",
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "import os\n",
        "\n",
        "# Set your HuggingFace token as an environment variable: export HF_TOKEN=\"your_token_here\"\n",
        "# Or use: huggingface-cli login\n",
        "hf_token = os.environ.get(\"HF_TOKEN\")\n",
        "if hf_token:\n",
        "    login(token=hf_token)\n",
        "else:\n",
        "    print(\"Warning: HF_TOKEN not found in environment variables. Using huggingface-cli login instead.\")\n",
        "    login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "71b2b040",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using vLLM for fast inference!\n"
          ]
        }
      ],
      "source": [
        "from vllm import LLM, SamplingParams\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "# model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "\n",
        "print(\"Using vLLM for fast inference!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "8fd5ba7f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 01-02 14:37:19 [utils.py:253] non-default args: {'trust_remote_code': True, 'max_model_len': 4096, 'disable_log_stats': True, 'model': 'Qwen/Qwen2.5-1.5B-Instruct'}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "55d12ac9c9e540bfaef4101b88e85570",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/660 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 01-02 14:37:20 [model.py:514] Resolved architecture: Qwen2ForCausalLM\n",
            "INFO 01-02 14:37:20 [model.py:1661] Using max model len 4096\n",
            "INFO 01-02 14:37:30 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=16384.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "09473164cc9a47019cc857a2e9e28333",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "97515c2288c14850b3aa9bccef4c9146",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ad17ff1f69b24793bd3cea84fb905f29",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3e9c2236ba9040dfa233e57fc140c3cf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3137f597ed9346309086962415c90b3b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[0;36m(EngineCore_DP0 pid=8439)\u001b[0;0m INFO 01-02 14:37:32 [core.py:93] Initializing a V1 LLM engine (v0.13.0) with config: model='Qwen/Qwen2.5-1.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-1.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=Qwen/Qwen2.5-1.5B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [16384], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=8439)\u001b[0;0m INFO 01-02 14:37:34 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.20.0.213:63015 backend=nccl\n",
            "\u001b[0;36m(EngineCore_DP0 pid=8439)\u001b[0;0m INFO 01-02 14:37:34 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "\u001b[0;36m(EngineCore_DP0 pid=8439)\u001b[0;0m INFO 01-02 14:37:35 [gpu_model_runner.py:3562] Starting to load model Qwen/Qwen2.5-1.5B-Instruct...\n",
            "\u001b[0;36m(EngineCore_DP0 pid=8439)\u001b[0;0m INFO 01-02 14:37:35 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aa0391267d0b4fc8991aa4ced20840bc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/3.09G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[0;36m(EngineCore_DP0 pid=8439)\u001b[0;0m INFO 01-02 14:37:46 [weight_utils.py:487] Time spent downloading weights for Qwen/Qwen2.5-1.5B-Instruct: 10.542827 seconds\n",
            "\u001b[0;36m(EngineCore_DP0 pid=8439)\u001b[0;0m INFO 01-02 14:37:46 [weight_utils.py:527] No model.safetensors.index.json found in remote.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b5ecf10074e74390ba7b67f00e6bf217",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[0;36m(EngineCore_DP0 pid=8439)\u001b[0;0m INFO 01-02 14:37:47 [default_loader.py:308] Loading weights took 0.70 seconds\n",
            "\u001b[0;36m(EngineCore_DP0 pid=8439)\u001b[0;0m INFO 01-02 14:37:47 [gpu_model_runner.py:3659] Model loading took 2.8876 GiB memory and 11.755601 seconds\n",
            "\u001b[0;36m(EngineCore_DP0 pid=8439)\u001b[0;0m INFO 01-02 14:37:57 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/0327b464e0/rank_0_0/backbone for vLLM's torch.compile\n",
            "\u001b[0;36m(EngineCore_DP0 pid=8439)\u001b[0;0m INFO 01-02 14:37:57 [backends.py:703] Dynamo bytecode transform time: 9.61 s\n",
            "\u001b[0;36m(EngineCore_DP0 pid=8439)\u001b[0;0m INFO 01-02 14:38:05 [backends.py:261] Cache the graph of compile range (1, 16384) for later use\n",
            "\u001b[0;36m(EngineCore_DP0 pid=8439)\u001b[0;0m INFO 01-02 14:38:12 [backends.py:278] Compiling a graph for compile range (1, 16384) takes 12.72 s\n",
            "\u001b[0;36m(EngineCore_DP0 pid=8439)\u001b[0;0m INFO 01-02 14:38:12 [monitor.py:34] torch.compile takes 22.33 s in total\n",
            "\u001b[0;36m(EngineCore_DP0 pid=8439)\u001b[0;0m INFO 01-02 14:38:13 [gpu_worker.py:375] Available KV cache memory: 117.20 GiB\n",
            "\u001b[0;36m(EngineCore_DP0 pid=8439)\u001b[0;0m INFO 01-02 14:38:13 [kv_cache_utils.py:1291] GPU KV cache size: 4,389,168 tokens\n",
            "\u001b[0;36m(EngineCore_DP0 pid=8439)\u001b[0;0m INFO 01-02 14:38:13 [kv_cache_utils.py:1296] Maximum concurrency for 4,096 tokens per request: 1071.57x\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[0;36m(EngineCore_DP0 pid=8439)\u001b[0;0m 2026-01-02 14:38:13,910 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...\n",
            "\u001b[0;36m(EngineCore_DP0 pid=8439)\u001b[0;0m 2026-01-02 14:38:13,927 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends\n",
            "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:02<00:00, 20.09it/s]\n",
            "Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:01<00:00, 40.10it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[0;36m(EngineCore_DP0 pid=8439)\u001b[0;0m INFO 01-02 14:38:18 [gpu_model_runner.py:4587] Graph capturing finished in 4 secs, took -0.57 GiB\n",
            "\u001b[0;36m(EngineCore_DP0 pid=8439)\u001b[0;0m INFO 01-02 14:38:18 [core.py:259] init engine (profile, create kv cache, warmup model) took 30.80 seconds\n",
            "INFO 01-02 14:38:19 [llm.py:360] Supported tasks: ['generate']\n",
            "vLLM model loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "# Load model using vLLM (much faster than transformers)\n",
        "llm = LLM(\n",
        "    model=model_name,\n",
        "    trust_remote_code=True,\n",
        "    max_model_len=4096,  # Adjust based on your GPU memory\n",
        "    gpu_memory_utilization=0.9,  # Use 90% of GPU memory\n",
        ")\n",
        "\n",
        "# Load tokenizer for formatting\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "\n",
        "# Set pad token if not present\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(f\"vLLM model loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "0c01e070",
      "metadata": {},
      "outputs": [],
      "source": [
        "sampling_params = SamplingParams(\n",
        "    temperature=0.0,  # Greedy decoding\n",
        "    max_tokens=1536,\n",
        "    top_p=1.0,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "02619716",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 1319 examples from GSM8K test set\n"
          ]
        }
      ],
      "source": [
        "# Load GSM8K test dataset\n",
        "from datasets import load_dataset\n",
        "import json\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load GSM8K test set\n",
        "dataset = load_dataset(\"openai/gsm8k\", \"main\", split=\"test\")\n",
        "print(f\"Loaded {len(dataset)} examples from GSM8K test set\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a90d1446",
      "metadata": {},
      "outputs": [],
      "source": [
        "trainset = load_dataset(\"openai/gsm8k\", \"main\", split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "d9c369d9",
      "metadata": {},
      "outputs": [],
      "source": [
        "math_dataset = load_dataset(\"qwedsacf/competition_math\", \"default\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "a6e3db59",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MATH dataset evaluation functions defined\n"
          ]
        }
      ],
      "source": [
        "# MATH Dataset evaluation functions (different from GSM8K)\n",
        "# MATH has complex answers: ordered pairs, fractions, expressions, etc.\n",
        "\n",
        "import re\n",
        "\n",
        "def extract_boxed_content(text):\n",
        "    \"\"\"Extract content from \\\\boxed{...} handling nested braces properly\"\"\"\n",
        "    if text is None:\n",
        "        return None\n",
        "    \n",
        "    # Find all \\boxed{ occurrences\n",
        "    pattern = r'\\\\boxed\\{'\n",
        "    matches = list(re.finditer(pattern, text))\n",
        "    \n",
        "    if not matches:\n",
        "        return None\n",
        "    \n",
        "    # Use the last \\boxed{ occurrence\n",
        "    match = matches[-1]\n",
        "    start = match.end()\n",
        "    \n",
        "    # Count braces to find matching closing brace\n",
        "    brace_count = 1\n",
        "    i = start\n",
        "    while i < len(text) and brace_count > 0:\n",
        "        if text[i] == '{':\n",
        "            brace_count += 1\n",
        "        elif text[i] == '}':\n",
        "            brace_count -= 1\n",
        "        i += 1\n",
        "    \n",
        "    if brace_count == 0:\n",
        "        return text[start:i-1].strip()\n",
        "    \n",
        "    return None\n",
        "\n",
        "def normalize_math_answer(answer):\n",
        "    \"\"\"Normalize mathematical expressions for comparison\n",
        "    \n",
        "    Handles:\n",
        "    - Ordered pairs: (4,5) or (4, 5)\n",
        "    - Fractions: \\\\frac{1}{2}, 1/2\n",
        "    - Algebraic expressions: 7(x+3)(x-3) vs 7(x+3) (x-3)\n",
        "    - LaTeX commands\n",
        "    - Whitespace normalization\n",
        "    \"\"\"\n",
        "    if answer is None:\n",
        "        return None\n",
        "    \n",
        "    answer = str(answer).strip()\n",
        "    \n",
        "    # Convert LaTeX fractions to standard form: \\\\frac{a}{b} -> (a)/(b)\n",
        "    answer = re.sub(r'\\\\frac\\{([^}]+)\\}\\{([^}]+)\\}', r'(\\1)/(\\2)', answer)\n",
        "    \n",
        "    # Remove \\\\text{...} but keep content\n",
        "    answer = re.sub(r'\\\\text\\{([^}]+)\\}', r'\\1', answer)\n",
        "    \n",
        "    # Remove other LaTeX commands\n",
        "    answer = re.sub(r'\\\\[a-zA-Z]+\\{', '', answer)\n",
        "    answer = re.sub(r'\\\\[a-zA-Z]+', '', answer)\n",
        "    \n",
        "    # Normalize ALL whitespace\n",
        "    answer = re.sub(r'\\s+', '', answer)\n",
        "    \n",
        "    # Remove dollar signs\n",
        "    answer = answer.replace('$', '')\n",
        "    \n",
        "    return answer.strip()\n",
        "\n",
        "def eval_math(generated, ground_truth):\n",
        "    \"\"\"Evaluate MATH dataset answer - handles complex mathematical expressions\n",
        "    \n",
        "    This function is compatible with the same calling convention as eval_gsm8k,\n",
        "    so it can be used in the same generation code.\n",
        "    \n",
        "    Args:\n",
        "        generated: The generated response text\n",
        "        ground_truth: The ground truth text (should contain \\\\boxed{answer})\n",
        "    \n",
        "    Returns:\n",
        "        is_correct: bool\n",
        "        gt_answer: str - Raw extracted answer (for display/output)\n",
        "        gen_answer: str - Raw extracted answer (for display/output)\n",
        "    \"\"\"\n",
        "    # Extract from boxed format (preserve full expressions)\n",
        "    gt_answer = extract_boxed_content(ground_truth)\n",
        "    gen_answer = extract_boxed_content(generated)\n",
        "    \n",
        "    # Normalize both answers for comparison\n",
        "    gt_answer_norm = normalize_math_answer(gt_answer)\n",
        "    gen_answer_norm = normalize_math_answer(gen_answer)\n",
        "    \n",
        "    # Compare normalized answers\n",
        "    is_correct = (gt_answer_norm == gen_answer_norm) if (gt_answer_norm and gen_answer_norm) else False\n",
        "    \n",
        "    # Return same format as eval_gsm8k for compatibility\n",
        "    # gt_answer and gen_answer are the RAW extracted expressions (not normalized)\n",
        "    return is_correct, gt_answer, gen_answer\n",
        "\n",
        "print(\"MATH dataset evaluation functions defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "c3f4edcd",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation functions defined\n"
          ]
        }
      ],
      "source": [
        "# GSM8K evaluation patterns\n",
        "gsm8k_pattern = re.compile(r\"\\n#### (.+)$\")  # Original GSM8K format\n",
        "boxed_pattern = re.compile(r\"\\\\boxed\\{([^}]+)\\}\")  # Qwen model output format\n",
        "hash_pattern = re.compile(r\"####\\s*(.+?)(?:\\n|$)\")  # #### format anywhere in text\n",
        "\n",
        "def extract_number(text):\n",
        "    \"\"\"Extract numeric value from text, ignoring units, currency symbols, etc.\n",
        "    \n",
        "    Examples:\n",
        "        '$65,000' -> '65000'\n",
        "        '\\$18' -> '18'\n",
        "        '18 dollars' -> '18'\n",
        "        '-5.5%' -> '-5.5'\n",
        "        '1/2' -> '0.5'\n",
        "    \"\"\"\n",
        "    if text is None:\n",
        "        return None\n",
        "    \n",
        "    text = str(text).strip()\n",
        "    \n",
        "    # Handle fractions (e.g., \"1/2\" -> \"0.5\")\n",
        "    fraction_match = re.match(r'^(-?\\d+)/(\\d+)$', text.replace(' ', ''))\n",
        "    if fraction_match:\n",
        "        numerator = float(fraction_match.group(1))\n",
        "        denominator = float(fraction_match.group(2))\n",
        "        if denominator != 0:\n",
        "            return str(numerator / denominator)\n",
        "    \n",
        "    # Remove currency symbols ($, €, £, ¥, etc.)\n",
        "    text = re.sub(r'[\\$€£¥₹]', '', text)\n",
        "    \n",
        "    # Remove common units (words)\n",
        "    text = re.sub(r'\\b(dollars?|cents?|years?|months?|days?|hours?|minutes?|seconds?|meters?|km|cm|kg|g|lbs?|oz|percent|%)\\b', '', text, flags=re.IGNORECASE)\n",
        "    \n",
        "    # Remove commas from numbers\n",
        "    text = text.replace(',', '')\n",
        "    \n",
        "    # Extract the number (handles negative, decimals)\n",
        "    # This pattern matches: optional minus, digits, optional decimal point and more digits\n",
        "    number_match = re.search(r'-?\\d+(?:\\.\\d+)?', text)\n",
        "    \n",
        "    if number_match:\n",
        "        return number_match.group(0)\n",
        "    \n",
        "    return None\n",
        "\n",
        "def extract_gsm8k_answer(text):\n",
        "    \"\"\"Extract answer from GSM8K format (#### answer at end)\"\"\"\n",
        "    match = re.search(gsm8k_pattern, text)\n",
        "    if not match:\n",
        "        return None\n",
        "    answer_text = match.group(1).strip()\n",
        "    return extract_number(answer_text)\n",
        "\n",
        "def extract_boxed_answer(text):\n",
        "    \"\"\"Extract answer from boxed format (\\\\boxed{answer})\"\"\"\n",
        "    matches = boxed_pattern.findall(text)\n",
        "    if not matches:\n",
        "        return None\n",
        "    answer_text = matches[-1].strip()\n",
        "    return extract_number(answer_text)\n",
        "\n",
        "def extract_hash_answer(text):\n",
        "    \"\"\"Extract answer from #### format (anywhere in text)\"\"\"\n",
        "    matches = hash_pattern.findall(text)\n",
        "    if not matches:\n",
        "        return None\n",
        "    answer_text = matches[-1].strip()\n",
        "    return extract_number(answer_text)\n",
        "\n",
        "def normalize_answer(answer):\n",
        "    \"\"\"Normalize numeric answer for comparison\"\"\"\n",
        "    if answer is None:\n",
        "        return None\n",
        "    \n",
        "    answer = str(answer).strip()\n",
        "    \n",
        "    # Try to convert to float and back to handle different representations\n",
        "    try:\n",
        "        # Convert to float to handle cases like \"5.0\" vs \"5\"\n",
        "        num = float(answer)\n",
        "        # If it's a whole number, convert to int to avoid \"5.0\" vs \"5\" mismatches\n",
        "        if num.is_integer():\n",
        "            return str(int(num))\n",
        "        else:\n",
        "            # Round to reasonable precision to avoid floating point issues\n",
        "            return f\"{num:.10g}\"  # 'g' format removes trailing zeros\n",
        "    except (ValueError, TypeError):\n",
        "        # If conversion fails, just return cleaned string\n",
        "        return answer.replace(',', '').strip()\n",
        "\n",
        "def eval_gsm8k(generated, ground_truth, extraction_func=None):\n",
        "    \"\"\"Evaluate GSM8K answer - handles multiple formats\n",
        "    \n",
        "    Args:\n",
        "        generated: The generated response text\n",
        "        ground_truth: The ground truth answer in GSM8K format\n",
        "        extraction_func: Function to extract answer from generated text\n",
        "                        If None, tries boxed, then hash, then gsm8k formats\n",
        "    \"\"\"\n",
        "    # Extract ground truth (GSM8K format with ####)\n",
        "    gt_answer = extract_boxed_answer(ground_truth)\n",
        "    \n",
        "    # Extract generated answer - try multiple formats\n",
        "    if extraction_func is not None:\n",
        "        gen_answer = extraction_func(generated)\n",
        "    else:\n",
        "        # Try boxed format first (for Qwen)\n",
        "        gen_answer = extract_boxed_answer(generated)\n",
        "        # If not found, try #### format\n",
        "        if gen_answer is None:\n",
        "            gen_answer = extract_hash_answer(generated)\n",
        "        # Last resort: try end-of-text GSM8K format\n",
        "        if gen_answer is None:\n",
        "            gen_answer = extract_gsm8k_answer(generated)\n",
        "    \n",
        "    # Normalize both answers\n",
        "    gt_answer_norm = normalize_answer(gt_answer)\n",
        "    gen_answer_norm = normalize_answer(gen_answer)\n",
        "    \n",
        "    # Compare normalized answers\n",
        "    is_correct = (gt_answer_norm == gen_answer_norm) if (gt_answer_norm and gen_answer_norm) else False\n",
        "    \n",
        "    return is_correct, gt_answer, gen_answer\n",
        "\n",
        "print(\"Evaluation functions defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "df47f8de",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "TESTING SYSTEM PROMPT\n",
            "================================================================================\n",
            "\n",
            "System Prompt:\n",
            "Please solve the problem step by step (separate steps with double newlines), but keep it short and put your final answer (do not include any other text or units) within \\boxed{}.\n",
            "\n",
            "Question:\n",
            "What are all values of $p$ such that for every $q>0$, we have   $$\\frac{3(pq^2+p^2q+3q^2+3pq)}{p+q}>2p^2q?$$ Express your answer in interval notation in decimal form.\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cb407db9b1574a779b8b37e5b9351b38",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f71df7c0de4d41338dfa3d21bf2d020b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "GENERATED RESPONSE:\n",
            "To solve the inequality \\(\\frac{3(pq^2 + p^2q + 3q^2 + 3pq)}{p+q} > 2p^2q\\) for all \\(q > 0\\), we start by simplifying the left-hand side.\n",
            "\n",
            "First, factor out \\(q\\) from the numerator:\n",
            "\\[\n",
            "pq^2 + p^2q + 3q^2 + 3pq = q(p^2 + pq + 3q + 3p)\n",
            "\\]\n",
            "Thus, the inequality becomes:\n",
            "\\[\n",
            "\\frac{3q(p^2 + pq + 3q + 3p)}{p+q} > 2p^2q\n",
            "\\]\n",
            "\n",
            "Next, we can divide both sides by \\(q\\) (since \\(q > 0\\)):\n",
            "\\[\n",
            "\\frac{3(p^2 + pq + 3q + 3p)}{p+q} > 2p^2\n",
            "\\]\n",
            "\n",
            "Now, multiply both sides by \\(p+q\\) to clear the fraction:\n",
            "\\[\n",
            "3(p^2 + pq + 3q + 3p) > 2p^2(p+q)\n",
            "\\]\n",
            "\n",
            "Expand and simplify the right-hand side:\n",
            "\\[\n",
            "3p^2 + 3pq + 9q + 9p > 2p^3 + 2p^2q\n",
            "\\]\n",
            "\n",
            "Rearrange all terms to one side:\n",
            "\\[\n",
            "3p^2 + 3pq + 9q + 9p - 2p^3 - 2p^2q > 0\n",
            "\\]\n",
            "\n",
            "Combine like terms:\n",
            "\\[\n",
            "-2p^3 + 3p^2 - 2p^2q + 3pq + 9q + 9p > 0\n",
            "\\]\n",
            "\n",
            "To find the values of \\(p\\) that satisfy this inequality for all \\(q > 0\\), we need to analyze the expression. Notice that the inequality must hold for all \\(q > 0\\). This suggests that the expression on the left must be positive for all \\(q > 0\\).\n",
            "\n",
            "Consider the expression:\n",
            "\\[\n",
            "-2p^3 + 3p^2 - 2p^2q + 3pq + 9q + 9p\n",
            "\\]\n",
            "\n",
            "For this to be positive for all \\(q > 0\\), the leading term \\(-2p^3\\) must be negative, which implies \\(p < 0\\). If \\(p \\geq 0\\), the term \\(-2p^3\\) would dominate and make the expression negative for sufficiently large \\(q\\).\n",
            "\n",
            "Thus, the only values of \\(p\\) that satisfy the inequality for all \\(q > 0\\) are:\n",
            "\\[\n",
            "p < 0\n",
            "\\]\n",
            "\n",
            "In interval notation, this is:\n",
            "\\[\n",
            "\\boxed{(-\\infty, 0)}\n",
            "\\]\n",
            "\n",
            "================================================================================\n",
            "Ground Truth Answer: [0,3)\n",
            "Generated Answer: (-\\infty, 0)\n",
            "Correct: ✗\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "test_system_prompt = \"\"\"Please solve the problem step by step (separate steps with double newlines), but keep it short and put your final answer (do not include any other text or units) within \\\\boxed{}.\"\"\"\n",
        "\n",
        "# test_system_prompt = \"Please solve step by step and put your final numerical answer (do not include any other text or units) after #### \"\n",
        "# test_system_prompt = \"Solve this problem step by step. Put your final answer in the format: #### [answer]\"\n",
        "\n",
        "# Choose a question to test (or use a custom one):\n",
        "test_question = math_dataset['train'][6]['problem']  # First question from GSM8K\n",
        "test_ground_truth = math_dataset['train'][6]['solution']\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"TESTING SYSTEM PROMPT\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nSystem Prompt:\\n{test_system_prompt}\\n\")\n",
        "print(f\"Question:\\n{test_question}\\n\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "# Create messages\n",
        "test_messages = [\n",
        "    {\"role\": \"system\", \"content\": test_system_prompt},\n",
        "    {\"role\": \"user\", \"content\": test_question}\n",
        "]\n",
        "\n",
        "# Format and generate\n",
        "test_prompt = tokenizer.apply_chat_template(test_messages, tokenize=False, add_generation_prompt=True)\n",
        "test_outputs = llm.generate([test_prompt], sampling_params)\n",
        "test_response = test_outputs[0].outputs[0].text.strip()\n",
        "\n",
        "# Evaluate\n",
        "is_correct, gt_ans, gen_ans = eval_math(test_response, test_ground_truth)\n",
        "\n",
        "print(f\"\\nGENERATED RESPONSE:\")\n",
        "print(test_response)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(f\"Ground Truth Answer: {gt_ans}\")\n",
        "print(f\"Generated Answer: {gen_ans}\")\n",
        "print(f\"Correct: {'✓' if is_correct else '✗'}\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "98033444",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preparing prompts for batch inference...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Preparing data: 100%|██████████| 1000/1000 [00:00<00:00, 9077.89it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Prepared 1000 prompts\n",
            "Starting batch inference with vLLM (this will be MUCH faster!)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "# Prepare all prompts first (for batch inference with vLLM)\n",
        "output_file = \"math_generations.jsonl\"\n",
        "\n",
        "# System prompt for math reasoning\n",
        "# system_prompt = \"Please reason step by step, and put your final answer within \\\\boxed{}.\"\n",
        "system_prompt = test_system_prompt\n",
        "\n",
        "print(\"Preparing prompts for batch inference...\")\n",
        "prompts = []\n",
        "questions = []\n",
        "ground_truths = []\n",
        "\n",
        "infer_set = math_dataset['train'].select(random.sample(range(len(math_dataset['train'])), 1000))\n",
        "\n",
        "for example in tqdm(infer_set, desc=\"Preparing data\"):\n",
        "    question = example['problem']\n",
        "    ground_truth_answer = example['solution']\n",
        "    \n",
        "    # Create messages in chat format\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": question}\n",
        "    ]\n",
        "    \n",
        "    # Format conversation using tokenizer's chat template\n",
        "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    \n",
        "    prompts.append(prompt)\n",
        "    questions.append(question)\n",
        "    ground_truths.append(ground_truth_answer)\n",
        "\n",
        "print(f\"\\nPrepared {len(prompts)} prompts\")\n",
        "print(f\"Starting batch inference with vLLM (this will be MUCH faster!)...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "ee42d329",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating responses with vLLM...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e78c73a4f9b943dea0178ad4266aa8c9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Adding requests:   0%|          | 0/1000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4706a23585494d9086cf9fe0ee5e2cdd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processed prompts:   0%|          | 0/1000 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated 1000 responses!\n"
          ]
        }
      ],
      "source": [
        "print(\"Generating responses with vLLM...\")\n",
        "outputs = llm.generate(prompts, sampling_params)\n",
        "\n",
        "print(f\"Generated {len(outputs)} responses!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "748662ed",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 1000/1000 [00:00<00:00, 29771.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "Evaluation Complete!\n",
            "==================================================\n",
            "Total examples: 1000\n",
            "Correct: 452\n",
            "Accuracy: 0.4520 (45.20%)\n",
            "Results saved to: math_generations.jsonl\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Process outputs and evaluate\n",
        "results = []\n",
        "correct_count = 0\n",
        "total_count = 0\n",
        "\n",
        "with open(output_file, 'w') as f:\n",
        "    for idx, output in enumerate(tqdm(outputs, desc=\"Evaluating\")):\n",
        "        try:\n",
        "            # Extract generated text\n",
        "            generated_response = output.outputs[0].text.strip()\n",
        "            \n",
        "            # Get corresponding question and ground truth\n",
        "            question = questions[idx]\n",
        "            ground_truth_answer = ground_truths[idx]\n",
        "            \n",
        "            # Evaluate\n",
        "            is_correct, gt_answer, gen_answer = eval_math(generated_response, ground_truth_answer)\n",
        "            \n",
        "            if is_correct:\n",
        "                correct_count += 1\n",
        "            total_count += 1\n",
        "            \n",
        "            # Save result in the same format as evaluate.py\n",
        "            result = {\n",
        "                \"question\": question,\n",
        "                \"ground_truth\": ground_truth_answer,\n",
        "                \"generated_response\": generated_response,\n",
        "                \"gt_answer\": gt_answer,\n",
        "                \"gen_answer\": gen_answer,\n",
        "                \"correct\": is_correct,\n",
        "                \"index\": idx\n",
        "            }\n",
        "            results.append(result)\n",
        "            \n",
        "            # Write to JSONL file\n",
        "            f.write(json.dumps(result) + '\\n')\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error at index {idx}: {e}\")\n",
        "            result = {\n",
        "                \"question\": questions[idx],\n",
        "                \"ground_truth\": ground_truths[idx],\n",
        "                \"generated_response\": \"\",\n",
        "                \"gt_answer\": None,\n",
        "                \"gen_answer\": None,\n",
        "                \"correct\": False,\n",
        "                \"index\": idx,\n",
        "                \"error\": str(e)\n",
        "            }\n",
        "            results.append(result)\n",
        "            f.write(json.dumps(result) + '\\n')\n",
        "            total_count += 1\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = correct_count / total_count if total_count > 0 else 0\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(f\"Evaluation Complete!\")\n",
        "print(f\"{'='*50}\")\n",
        "print(f\"Total examples: {total_count}\")\n",
        "print(f\"Correct: {correct_count}\")\n",
        "print(f\"Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "print(f\"Results saved to: {output_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "310a4d56",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "Example Results (first 3):\n",
            "================================================================================\n",
            "\n",
            "--- Example 1 ---\n",
            "Question: What is the unique three-digit positive integer $x$ satisfying $$100x\\equiv 1\\pmod{997}~?$$...\n",
            "Ground Truth Answer: 668\n",
            "Generated Answer: 960\n",
            "Correct: False\n",
            "Full Generation: To find the unique three-digit positive integer \\( x \\) that satisfies the congruence \\( 100x \\equiv 1 \\pmod{997} \\), we need to find the modular inverse of 100 modulo 997. This means we need to find an integer \\( x \\) such that:\n",
            "\n",
            "\\[ 100x \\equiv 1 \\pmod{997} \\]\n",
            "\n",
            "This can be solved using the Extended Euclidean Algorithm, which finds solutions to equations of the form:\n",
            "\n",
            "\\[ ax + by = \\gcd(a, b) \\]\n",
            "\n",
            "For our case, \\( a = 100 \\) and \\( b = 997 \\). Since 100 and 997 are coprime (their greatest common d...\n",
            "\n",
            "\n",
            "--- Example 2 ---\n",
            "Question: What positive five-digit whole number with an 8 in the ten-thousands place is a perfect cube?...\n",
            "Ground Truth Answer: 85,\\!184\n",
            "Generated Answer: 85184\n",
            "Correct: False\n",
            "Full Generation: To find a positive five-digit whole number with an 8 in the ten-thousands place that is a perfect cube, we need to consider the form of a perfect cube and the constraints given by the problem.\n",
            "\n",
            "1. **Identify the form of the number**: The number must be a five-digit number, so it ranges from 10000 to 99999. The ten-thousands place is fixed as 8, so the number can be written as 8xxxx, where x can be any digit from 0 to 9.\n",
            "\n",
            "2. **Determine the cube root**: We need to find a number \\( n \\) such that ...\n",
            "\n",
            "\n",
            "--- Example 3 ---\n",
            "Question: Let $n$ be the number of ordered quadruples $(x_1,x_2,x_3,x_4)$ of positive odd integers that satisf...\n",
            "Ground Truth Answer: 196\n",
            "Generated Answer: 196\n",
            "Correct: True\n",
            "Full Generation: To solve the problem, we need to find the number of ordered quadruples \\((x_1, x_2, x_3, x_4)\\) of positive odd integers that satisfy the equation \\(\\sum_{i=1}^4 x_i = 98\\).\n",
            "\n",
            "1. **Expressing the problem in terms of even numbers:**\n",
            "   Since each \\(x_i\\) is a positive odd integer, we can write each \\(x_i\\) as \\(x_i = 2y_i + 1\\) where \\(y_i\\) is a non-negative integer. This transformation allows us to convert the problem into finding the number of solutions to \\(\\sum_{i=1}^4 (2y_i + 1) = 98\\).\n",
            "\n",
            "2. ...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Display some example results\n",
        "print(\"=\"*80)\n",
        "print(\"Example Results (first 3):\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for i, result in enumerate(results[:3]):\n",
        "    print(f\"\\n--- Example {i+1} ---\")\n",
        "    print(f\"Question: {result['question'][:100]}...\")\n",
        "    print(f\"Ground Truth Answer: {result['gt_answer']}\")\n",
        "    print(f\"Generated Answer: {result['gen_answer']}\")\n",
        "    print(f\"Correct: {result['correct']}\")\n",
        "    print(f\"Full Generation: {result['generated_response'][:500]}...\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "678543eb",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "SYNTHETIC DATA GENERATION CONFIGURATION\n",
            "================================================================================\n",
            "Max retries per question: 5\n",
            "System prompt: Please solve the problem step by step (separate steps with double newlines), but keep it short and put your final answer (do not include any other text or units) within \\boxed{}.\n",
            "Output file: qwen30b_math_synthetic_data.jsonl\n",
            "Max examples: All\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# ========================================================================\n",
        "# SYNTHETIC DATA GENERATION WITH RETRY MECHANISM\n",
        "# ========================================================================\n",
        "\n",
        "# Configuration\n",
        "MAX_RETRIES = 5  # Maximum number of generation attempts per question\n",
        "SYNTHETIC_OUTPUT_FILE = \"qwen30b_math_synthetic_data.jsonl\"\n",
        "# synthetic_system_prompt = r\"Please reason step by step, and put your final answer within \\boxed{}.\"\n",
        "synthetic_system_prompt = test_system_prompt\n",
        "\n",
        "# You can also limit the number of examples to process (None = all)\n",
        "MAX_EXAMPLES_FOR_SYNTHETIC = None  # Set to e.g. 100 for testing, None for full dataset\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"SYNTHETIC DATA GENERATION CONFIGURATION\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Max retries per question: {MAX_RETRIES}\")\n",
        "print(f\"System prompt: {synthetic_system_prompt}\")\n",
        "print(f\"Output file: {SYNTHETIC_OUTPUT_FILE}\")\n",
        "print(f\"Max examples: {MAX_EXAMPLES_FOR_SYNTHETIC or 'All'}\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f7cf47b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Starting BATCH synthetic data generation for 12500 examples...\n",
            "Using batch inference with retry (max 5 attempts)\n",
            "\n",
            "\n",
            "Attempt 1/5: Generating for 12500 questions...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cafb5abd2316433498201cd021a2b024",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Adding requests:   0%|          | 0/12500 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dc0d396ad8a84b60ab635db31eef5b4a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processed prompts:   0%|          | 0/12500 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  -> 5730/12500 correct (45.84%)\n",
            "\n",
            "Attempt 2/5: Generating for 6770 questions...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a1f047d21d084d6d910ed711322950e5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Adding requests:   0%|          | 0/6770 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d2de0262c23f48af9348bcb34b507680",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processed prompts:   0%|          | 0/6770 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mFailed to restart the Kernel. \n",
            "\u001b[1;31mrequest to https://ta-01kdzdf7w5fbdf6k484pr6dp67-8888.wo-w5yc6ncmg3q4uepbxrp2p5rk6.w.modal.host/api/kernels/8ddb403a-3e68-4d63-a769-2f9f4b01d233/restart?1767366841179 failed, reason: Client network socket disconnected before secure TLS connection was established. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# Generate synthetic data with BATCH retry mechanism (FAST!)\n",
        "import time\n",
        "\n",
        "# Prepare dataset\n",
        "synthetic_dataset = math_dataset['train'] if MAX_EXAMPLES_FOR_SYNTHETIC is None else math_dataset['train'].select(range(min(MAX_EXAMPLES_FOR_SYNTHETIC, len(math_dataset['train']))))\n",
        "\n",
        "print(f\"\\nStarting BATCH synthetic data generation for {len(synthetic_dataset)} examples...\")\n",
        "print(f\"Using batch inference with retry (max {MAX_RETRIES} attempts)\\n\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Prepare all data\n",
        "all_questions = []\n",
        "all_ground_truths = []\n",
        "all_prompts = []\n",
        "\n",
        "for example in synthetic_dataset:\n",
        "    question = example['problem']\n",
        "    ground_truth = example['solution']\n",
        "    \n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": synthetic_system_prompt},\n",
        "        {\"role\": \"user\", \"content\": question}\n",
        "    ]\n",
        "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    \n",
        "    all_questions.append(question)\n",
        "    all_ground_truths.append(ground_truth)\n",
        "    all_prompts.append(prompt)\n",
        "\n",
        "# Track results\n",
        "results = [None] * len(synthetic_dataset)  # Store successful responses\n",
        "pending_indices = list(range(len(synthetic_dataset)))  # Indices still needing correct answer\n",
        "total_attempts = 0\n",
        "attempt_count = 0\n",
        "\n",
        "# Batch generation with retry\n",
        "while pending_indices and attempt_count < MAX_RETRIES:\n",
        "    attempt_count += 1\n",
        "    print(f\"\\nAttempt {attempt_count}/{MAX_RETRIES}: Generating for {len(pending_indices)} questions...\")\n",
        "    \n",
        "    # Get prompts for pending questions\n",
        "    pending_prompts = [all_prompts[i] for i in pending_indices]\n",
        "    \n",
        "    # Batch generate\n",
        "    outputs = llm.generate(pending_prompts, sampling_params)\n",
        "    total_attempts += len(pending_prompts)\n",
        "    \n",
        "    # Evaluate results\n",
        "    new_pending = []\n",
        "    for i, output_idx in enumerate(pending_indices):\n",
        "        response = outputs[i].outputs[0].text.strip()\n",
        "        \n",
        "        # Check if correct\n",
        "        is_correct, gt_ans, gen_ans = eval_gsm8k(response, all_ground_truths[output_idx])\n",
        "        \n",
        "        if is_correct:\n",
        "            # Store successful result\n",
        "            results[output_idx] = response\n",
        "        else:\n",
        "            # Still needs correct answer\n",
        "            new_pending.append(output_idx)\n",
        "    \n",
        "    pending_indices = new_pending\n",
        "    success_count = sum(1 for r in results if r is not None)\n",
        "    print(f\"  -> {success_count}/{len(synthetic_dataset)} correct ({success_count/len(synthetic_dataset)*100:.2f}%)\")\n",
        "\n",
        "# Create final dataset (only successful examples)\n",
        "synthetic_data = []\n",
        "success_count = 0\n",
        "failed_count = 0\n",
        "\n",
        "for idx in range(len(synthetic_dataset)):\n",
        "    if results[idx] is not None:\n",
        "        # Extract ground truth numeric answer for easy comparison\n",
        "        ground_truth_answer = extract_gsm8k_answer(all_ground_truths[idx])\n",
        "        \n",
        "        synthetic_entry = {\n",
        "            \"question\": all_questions[idx],\n",
        "            \"answer\": all_ground_truths[idx],  # Full GSM8K answer with #### format\n",
        "            \"ground_truth\": ground_truth_answer,  # Extracted numeric answer\n",
        "            \"deepseek_response\": results[idx]\n",
        "        }\n",
        "        synthetic_data.append(synthetic_entry)\n",
        "        success_count += 1\n",
        "    else:\n",
        "        failed_count += 1\n",
        "\n",
        "elapsed_time = time.time() - start_time\n",
        "\n",
        "# Print statistics\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"SYNTHETIC DATA GENERATION COMPLETE\")\n",
        "print(f\"{'='*80}\")\n",
        "print(f\"Total examples processed: {len(synthetic_dataset)}\")\n",
        "print(f\"Successful (found correct answer): {success_count} ({success_count/len(synthetic_dataset)*100:.2f}%)\")\n",
        "print(f\"Failed (no correct answer in {MAX_RETRIES} attempts): {failed_count} ({failed_count/len(synthetic_dataset)*100:.2f}%)\")\n",
        "print(f\"Total generation attempts: {total_attempts}\")\n",
        "print(f\"Average attempts per question: {total_attempts/len(synthetic_dataset):.2f}\")\n",
        "print(f\"Time elapsed: {elapsed_time:.2f}s ({elapsed_time/60:.2f} minutes)\")\n",
        "print(f\"Speed: {len(synthetic_dataset)/elapsed_time:.2f} examples/second\")\n",
        "print(f\"{'='*80}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "b8930bfb",
      "metadata": {},
      "outputs": [],
      "source": [
        "synthetic_data = []\n",
        "success_count = 0\n",
        "failed_count = 0\n",
        "\n",
        "for idx in range(len(synthetic_dataset)):\n",
        "    if results[idx] is not None:\n",
        "        # Extract ground truth numeric answer for easy comparison\n",
        "        ground_truth_answer = extract_boxed_answer(all_ground_truths[idx])\n",
        "        ground_truth_answer = normalize_math_answer(ground_truth_answer)\n",
        "        \n",
        "        synthetic_entry = {\n",
        "            \"question\": all_questions[idx],\n",
        "            \"answer\": all_ground_truths[idx],  # Full GSM8K answer with #### format\n",
        "            \"ground_truth\": ground_truth_answer,  # Extracted numeric answer\n",
        "            \"deepseek_response\": results[idx]\n",
        "        }\n",
        "        synthetic_data.append(synthetic_entry)\n",
        "        success_count += 1\n",
        "    else:\n",
        "        failed_count += 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "875b7ecb",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Saving synthetic data to qwen30b_math_synthetic_data.jsonl...\n",
            "✓ Saved 10032 successful entries to qwen30b_math_synthetic_data.jsonl\n",
            "\n",
            "File format (same as GSM8K + additional fields):\n",
            "  - question: The math problem\n",
            "  - answer: Full ground truth with #### format\n",
            "  - ground_truth: Extracted numeric answer (for easy comparison)\n",
            "  - deepseek_response: Model's correct response\n"
          ]
        }
      ],
      "source": [
        "# Save synthetic data to JSONL file\n",
        "print(f\"\\nSaving synthetic data to {SYNTHETIC_OUTPUT_FILE}...\")\n",
        "\n",
        "with open(SYNTHETIC_OUTPUT_FILE, 'w') as f:\n",
        "    for entry in synthetic_data:\n",
        "        f.write(json.dumps(entry) + '\\n')\n",
        "\n",
        "print(f\"✓ Saved {len(synthetic_data)} successful entries to {SYNTHETIC_OUTPUT_FILE}\")\n",
        "print(f\"\\nFile format (same as GSM8K + additional fields):\")\n",
        "print(f\"  - question: The math problem\")\n",
        "print(f\"  - answer: Full ground truth with #### format\")\n",
        "print(f\"  - ground_truth: Extracted numeric answer (for easy comparison)\")\n",
        "print(f\"  - deepseek_response: Model's correct response\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19563e1a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display examples of synthetic data\n",
        "print(\"=\"*80)\n",
        "print(\"SYNTHETIC DATA EXAMPLES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Show first few successful examples\n",
        "if len(synthetic_data) > 0:\n",
        "    print(f\"\\n📊 Example 1:\")\n",
        "    print(\"-\"*80)\n",
        "    example = synthetic_data[0]\n",
        "    print(f\"Question: {example['question'][:200]}...\")\n",
        "    print(f\"\\nGround Truth (extracted): {example['ground_truth']}\")\n",
        "    print(f\"\\nFull Answer: {example['answer'][:100]}...\")\n",
        "    print(f\"\\nDeepseek Response: {example['deepseek_response'][:400]}...\")\n",
        "    \n",
        "    if len(synthetic_data) > 1:\n",
        "        print(f\"\\n\\n📊 Example 2:\")\n",
        "        print(\"-\"*80)\n",
        "        example = synthetic_data[1]\n",
        "        print(f\"Question: {example['question'][:200]}...\")\n",
        "        print(f\"\\nGround Truth (extracted): {example['ground_truth']}\")\n",
        "        print(f\"\\nFull Answer: {example['answer'][:100]}...\")\n",
        "        print(f\"\\nDeepseek Response: {example['deepseek_response'][:400]}...\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3b650f3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# How to load and use the synthetic data\n",
        "print(\"=\"*80)\n",
        "print(\"HOW TO USE THE SYNTHETIC DATA\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\"\"\n",
        "The synthetic data has been saved to: {SYNTHETIC_OUTPUT_FILE}\n",
        "\n",
        "Format (same as GSM8K dataset + additional fields):\n",
        "{{\n",
        "    \"question\": \"Math problem text\",\n",
        "    \"answer\": \"Solution with #### final_answer\",\n",
        "    \"ground_truth\": \"18\",  // Extracted numeric answer\n",
        "    \"deepseek_response\": \"Model's correct reasoning with \\\\boxed{{answer}}\"\n",
        "}}\n",
        "\n",
        "Only successful examples (where model got correct answer) are included.\n",
        "\n",
        "Example usage:\n",
        "\"\"\")\n",
        "\n",
        "print(\"# Load the synthetic data:\")\n",
        "print(\"from datasets import load_dataset\")\n",
        "print(f\"synthetic_ds = load_dataset('json', data_files='{SYNTHETIC_OUTPUT_FILE}')['train']\")\n",
        "print()\n",
        "print(\"# Access fields:\")\n",
        "print(\"for example in synthetic_ds:\")\n",
        "print(\"    question = example['question']\")\n",
        "print(\"    full_answer = example['answer']  # Full GSM8K format\")\n",
        "print(\"    ground_truth = example['ground_truth']  # Just the number\")\n",
        "print(\"    model_response = example['deepseek_response']\")\n",
        "print()\n",
        "print(\"# The ground_truth field makes comparison easier:\")\n",
        "print(\"# - No need to parse #### format\")\n",
        "print(\"# - Direct numeric comparison\")\n",
        "print(\"# - Useful for analysis and evaluation\")\n",
        "print()\n",
        "print(\"# Use cases:\")\n",
        "print(\"# 1. Fine-tuning - Use as training data\")\n",
        "print(\"# 2. Data augmentation - Add to existing datasets\")\n",
        "print(\"# 3. Distillation - Train smaller models\")\n",
        "print(\"# 4. Analysis - Study model reasoning patterns\")\n",
        "print(\"# 5. Evaluation - Easy comparison with ground_truth field\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
